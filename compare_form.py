import pandas as pd
import matplotlib.pyplot as plt

# ==========================================
# 1. Data Setup (Full 100 Epochs Data)
# ==========================================

# We keep the full data here, but we will SLICE it later for plotting
full_epochs = list(range(1, 101))

# Dummy example data (replace with your actual numbers)
# Each list should have the same length as epochs
standard_transformer = {
    "train_loss": [
        8.2162,7.3298,7.1527,6.9992,6.8713,6.7736,6.6927,6.6257,6.5626,6.5032,
        6.4449,6.3907,6.3385,6.29,6.2445,6.2032,6.1639,6.1317,6.0993,6.0645,
        6.0323,6.0008,5.9711,5.9453,5.9181,5.8929,5.8759,5.8521,5.8275,5.8016,
        5.7814,5.7595,5.7387,5.7155,5.6947,5.6735,5.6533,5.6331,5.6149,5.5976,
        5.5796,5.5631,5.5462,5.5323,5.5199,5.5088,5.4951,5.4826,5.4668,5.4525,
        5.44,5.4287,5.4169,5.4073,5.399,5.3894,5.3794,5.3701,5.3615,5.3538,
        5.3448,5.3353,5.3273,5.3204,5.3128,5.3065,5.2999,5.294,5.2895,5.284,
        5.2789,5.2731,5.2685,5.2645,5.26,5.2559,5.2521,5.2491,5.2464,5.2437,
        5.2404,5.2385,5.2371,5.2351,5.2338,5.2318,5.2305,5.2294,5.2279,5.2268,
        5.226,5.2248,5.2238,5.2233,5.2229,5.2235,5.2231,5.2228,5.2226,5.2221
    ],
    "train_ppl": [
        3700.59,1525.04,1277.51,1095.78,964.17,874.49,806.53,754.26,708.14,667.28,
        629.47,596.25,565.97,539.13,515.19,494.34,475.29,460.21,445.57,430.3,
        416.68,403.77,391.92,381.96,371.69,362.47,356.35,347.95,339.5,330.84,
        324.21,317.19,310.67,303.54,297.28,291.05,285.23,279.54,274.48,269.78,
        264.96,260.63,256.25,252.72,249.61,246.86,243.49,240.47,236.69,233.33,
        230.45,227.85,225.18,223.03,221.18,219.07,216.9,214.89,213.04,211.42,
        209.51,207.54,205.89,204.46,202.92,201.65,200.31,199.14,198.24,197.15,
        196.16,195.01,194.12,193.35,192.48,191.69,190.96,190.39,189.89,189.37,
        188.75,188.39,188.12,187.75,187.51,187.13,186.88,186.68,186.39,186.19,
        186.05,185.82,185.64,185.55,185.47,185.58,185.51,185.45,185.42,185.33
    ],
    "valid_loss": [
        7.1521,6.9636,6.8253,6.7013,6.6011,6.541,6.5008,6.4557,6.4133,6.3802,
        6.3284,6.2867,6.2527,6.2232,6.19,6.1618,6.1387,6.1186,6.1004,6.0785,
        6.0773,6.0589,6.0372,6.0233,6.0069,6.0011,5.9996,5.9848,5.9575,5.9425,
        5.9363,5.9158,5.9124,5.9045,5.8953,5.8799,5.8748,5.8687,5.8635,5.8545,
        5.8539,5.8473,5.8427,5.8363,5.827,5.8207,5.824,5.8169,5.8107,5.801,
        5.7963,5.7937,5.7925,5.7972,5.7935,5.7889,5.7868,5.7821,5.7757,5.7711,
        5.7713,5.767,5.7601,5.7593,5.7594,5.7594,5.7556,5.7517,5.751,5.7508,
        5.7465,5.7442,5.7413,5.7384,5.7369,5.7363,5.7355,5.7349,5.7336,5.733,
        5.7323,5.732,5.7318,5.7307,5.7292,5.7292,5.7282,5.728,5.7277,5.7273,
        5.7271,5.7268,5.7267,5.7266,5.7264,5.7264,5.7263,5.7263,5.7263,5.7263
    ],
    "valid_ppl": [
        1276.76,1057.45,920.85,813.43,735.89,693.01,665.67,636.33,609.89,590.02,
        560.26,537.36,519.4,504.32,487.84,474.28,463.46,454.24,446.04,436.39,
        435.87,427.91,418.71,412.96,406.24,403.89,403.28,397.36,386.64,380.89,
        378.54,370.85,369.61,366.67,363.32,357.77,355.97,353.77,351.96,348.79,
        348.6,346.29,344.7,342.51,339.34,337.22,338.31,335.94,333.85,330.64,
        329.08,328.23,327.82,329.36,328.16,326.66,325.98,324.44,322.38,320.89,
        320.95,319.59,317.39,317.14,317.15,317.16,315.96,314.72,314.49,314.44,
        313.1,312.38,311.46,310.57,310.12,309.93,309.68,309.47,309.08,308.91,
        308.69,308.6,308.54,308.19,307.72,307.74,307.42,307.37,307.25,307.13,
        307.09,307,306.95,306.92,306.87,306.86,306.83,306.83,306.83,306.83
    ],
}

#standard_transformer = {
#    "train_loss": [
#        8.7722, 7.6075, 7.3584, 7.2466, 7.1527, 7.0624, 6.979, 6.9022, 6.8335, 6.7764,
#        6.7266, 6.6839, 6.6453, 6.6079, 6.5737, 6.5415, 6.5085, 6.4748, 6.4434, 6.4126,
#        6.3829, 6.3539, 6.3308, 6.3115, 6.2873, 6.2639, 6.2408, 6.22, 6.197, 6.1752,
#        6.1575, 6.1443, 6.1307, 6.1134, 6.0926, 6.0769, 6.0586, 6.0435, 6.0288, 6.0161,
#        6.0035, 5.9918, 5.983, 5.9759, 5.9655, 5.9534, 5.9445, 5.933, 5.9206, 5.9074,
#        5.8955, 5.8856, 5.8749, 5.8668, 5.8578, 5.8505, 5.8431, 5.8367, 5.8309, 5.8254,
#        5.8162, 5.8088, 5.8026, 5.7967, 5.7915, 5.7858, 5.7814, 5.7774, 5.7723, 5.7687,
#        5.7648, 5.7607, 5.7579, 5.7551, 5.752, 5.749, 5.7463, 5.7438, 5.7422, 5.7396,
#        5.7377, 5.7358, 5.7342, 5.7328, 5.7315, 5.7295, 5.7286, 5.7276, 5.7273, 5.7264,
#        5.7252, 5.7246, 5.7243, 5.7239, 5.7239, 5.7244, 5.7244, 5.7237, 5.7236, 5.7237
#    ],
#    "train_ppl": [
#        6452.47, 2013.15, 1569.26, 1403.3, 1277.58, 1167.21, 1073.88, 994.42, 928.46, 876.87,
#        834.31, 799.39, 769.13, 740.9, 716.04, 693.33, 670.83, 648.62, 628.51, 609.49,
#        591.63, 574.73, 561.61, 550.89, 537.72, 525.25, 513.29, 502.69, 491.28, 480.69,
#        472.27, 466.07, 459.74, 451.86, 442.58, 435.69, 427.76, 421.35, 415.2, 409.96,
#        404.83, 400.12, 396.61, 393.82, 389.75, 385.08, 381.65, 377.29, 372.63, 367.75,
#        363.38, 359.81, 355.98, 353.11, 349.95, 347.4, 344.85, 342.66, 340.67, 338.81,
#        335.7, 333.23, 331.16, 329.2, 327.51, 325.63, 324.22, 322.92, 321.27, 320.11,
#        318.86, 317.59, 316.67, 315.81, 314.81, 313.87, 313.02, 312.26, 311.75, 310.94,
#        310.34, 309.76, 309.28, 308.83, 308.43, 307.81, 307.55, 307.22, 307.13, 306.87,
#        306.48, 306.31, 306.22, 306.1, 306.09, 306.24, 306.26, 306.04, 306.0, 306.05
#    ],
#    "valid_loss": [
#        7.614, 7.1515, 7.0334, 6.9426, 6.8637, 6.7867, 6.7218, 6.6545, 6.6033, 6.5693,
#        6.5406, 6.5104, 6.4958, 6.4817, 6.4657, 6.4466, 6.4196, 6.3965, 6.3772, 6.3504,
#        6.3334, 6.3179, 6.2966, 6.2943, 6.2777, 6.2595, 6.2376, 6.2309, 6.2116, 6.2039,
#        6.1895, 6.1862, 6.1761, 6.1532, 6.1471, 6.1372, 6.1333, 6.1247, 6.1145, 6.1029,
#        6.1021, 6.0994, 6.0946, 6.0862, 6.0838, 6.0681, 6.0733, 6.0584, 6.0552, 6.0461,
#        6.0374, 6.0361, 6.0306, 6.0269, 6.0227, 6.0179, 6.0156, 6.0137, 6.0102, 6.0111,
#        6.0084, 6.0034, 6.0026, 6.0012, 5.9988, 5.9969, 5.997, 5.9947, 5.9932, 5.991,
#        5.9893, 5.9865, 5.9847, 5.9829, 5.9811, 5.9801, 5.9792, 5.9787, 5.9789, 5.9783,
#        5.9781, 5.977, 5.9752, 5.974, 5.9737, 5.9731, 5.9725, 5.9726, 5.9721, 5.9719,
#        5.9717, 5.9716, 5.9714, 5.9713, 5.9712, 5.9711, 5.9711, 5.9711, 5.9711, 5.9711
#    ],
#    "valid_ppl": [
#        2026.31, 1276.07, 1133.85, 1035.51, 956.85, 886.03, 830.27, 776.28, 737.53, 712.88,
#        692.68, 672.12, 662.38, 653.09, 642.72, 630.54, 613.73, 599.71, 588.27, 572.74,
#        563.09, 554.41, 542.74, 541.49, 532.55, 522.94, 511.61, 508.19, 498.5, 494.7,
#        487.62, 485.99, 481.11, 470.2, 467.37, 462.76, 460.97, 456.99, 452.37, 447.16,
#        446.79, 445.58, 443.45, 439.75, 438.71, 431.84, 434.11, 427.67, 426.34, 422.46,
#        418.79, 418.25, 415.98, 414.44, 412.7, 410.7, 409.79, 409.01, 407.58, 407.94,
#        406.84, 404.79, 404.48, 403.9, 402.93, 402.17, 402.23, 401.32, 400.71, 399.8,
#        399.14, 398.02, 397.29, 396.6, 395.87, 395.49, 395.12, 394.91, 395.0, 394.79,
#        394.7, 394.24, 393.53, 393.06, 392.94, 392.71, 392.49, 392.54, 392.34, 392.23,
#        392.19, 392.15, 392.06, 392.0, 391.98, 391.95, 391.94, 391.93, 391.93, 391.93
#    ],
#}

hierarchical_transformer_weighted_sum = {
    "train_loss": [
        23.6638,8.3151,7.4824,7.2377,7.0663,6.9228,6.8044,6.7048,6.6215,6.5595,
        6.5021,6.4449,6.394,6.3507,6.3132,6.2788,6.2415,6.2034,6.1684,6.1401,
        6.1098,6.0808,6.0521,6.023,5.992,5.9638,5.9392,5.9148,5.8888,5.8626,
        5.8452,5.8278,5.8117,5.79,5.7706,5.7583,5.739,5.7246,5.7113,5.6948,
        5.6787,5.6656,5.6486,5.6325,5.6193,5.6057,5.5912,5.5777,5.5629,5.551,
        5.5398,5.5302,5.5186,5.5089,5.5001,5.4911,5.4814,5.4721,5.4624,5.4551,
        5.4459,5.4384,5.4285,5.4212,5.4145,5.4059,5.3992,5.3944,5.3877,5.3817,
        5.3763,5.3717,5.3645,5.3581,5.3525,5.3474,5.3429,5.3382,5.3338,5.3292,
        5.3262,5.3224,5.3193,5.3167,5.3147,5.3112,5.3096,5.3073,5.3058,5.3048,
        5.3032,5.302,5.3,5.2995,5.2984,5.2971,5.2971,5.2966,5.2963,5.2964
    ],
    "train_ppl": [
        485165195.4,4084.96,1776.49,1390.83,1171.83,1015.18,901.8,816.31,751.03,705.9,
        666.52,629.49,598.24,572.87,551.78,533.12,513.6,494.41,477.43,464.11,
        450.26,437.36,424.99,412.83,400.22,389.09,379.63,370.47,360.97,351.63,
        345.58,339.63,334.19,327,320.73,316.81,310.74,306.31,302.27,297.33,
        292.56,288.76,283.89,279.36,275.71,271.98,268.07,264.48,260.57,257.49,
        254.63,252.19,249.28,246.88,244.73,242.52,240.17,237.96,235.66,233.95,
        231.82,230.06,227.8,226.15,224.63,222.71,221.23,220.16,218.69,217.4,
        216.21,215.23,213.69,212.32,211.13,210.07,209.12,208.14,207.22,206.28,
        205.65,204.87,204.24,203.72,203.3,202.58,202.27,201.81,201.5,201.3,
        200.99,200.74,200.35,200.23,200.02,199.76,199.75,199.65,199.6,199.61
    ],
    "valid_loss": [
        8.6563,7.2699,7.0405,6.8825,6.75,6.6357,6.5411,6.4687,6.4265,6.374,
        6.3244,6.2944,6.2739,6.2406,6.2198,6.2036,6.1785,6.1487,6.1286,6.1207,
        6.1024,6.0928,6.0649,6.051,6.0341,6.0366,6.0284,6.0149,5.9906,5.9822,
        5.9837,5.9763,5.9741,5.9568,5.9534,5.939,5.9431,5.9461,5.9287,5.9343,
        5.9293,5.913,5.9101,5.9087,5.896,5.9112,5.9012,5.8906,5.8884,5.8892,
        5.8811,5.8826,5.8803,5.8751,5.8677,5.8667,5.8649,5.8581,5.8611,5.8547,
        5.852,5.8615,5.8513,5.8583,5.8548,5.8496,5.8534,5.8494,5.8452,5.8502,
        5.8488,5.8408,5.8391,5.8369,5.8346,5.8293,5.8301,5.8307,5.8296,5.8286,
        5.828,5.8255,5.8259,5.825,5.8243,5.8229,5.8248,5.824,5.8229,5.8224,
        5.8215,5.8212,5.8206,5.8204,5.8204,5.8198,5.8195,5.8191,5.8191,5.8191
    ],
    "valid_ppl": [
        5745.99,1436.35,1141.91,975.05,854.06,761.82,693.05,644.61,618.01,586.39,
        558.02,541.55,530.55,513.16,502.62,494.52,482.25,468.13,458.81,455.2,
        446.94,442.65,430.49,424.53,417.41,418.46,415.04,409.47,399.67,396.3,
        396.91,393.96,393.1,386.36,385.08,379.54,381.11,382.25,375.68,377.76,
        375.89,369.83,368.75,368.21,363.57,369.16,365.48,361.61,360.82,361.13,
        358.2,358.74,357.93,356.07,353.45,353.07,352.44,350.04,351.1,348.89,
        347.92,351.24,347.69,350.12,348.9,347.09,348.43,347.02,345.59,347.29,
        346.83,344.06,343.47,342.7,341.94,340.12,340.39,340.59,340.23,339.88,
        339.69,338.82,338.95,338.67,338.43,337.96,338.6,338.33,337.94,337.79,
        337.49,337.37,337.16,337.1,337.12,336.9,336.8,336.68,336.67,336.66
    ],
}

hierarchical_transformer_C2P_P2C = {
    "train_loss": [
        8.1714, 7.2955, 7.0903, 6.9167, 6.7798, 6.6763, 6.598, 6.5311, 6.471,
        6.41, 6.3523, 6.2995, 6.2482, 6.1965, 6.1481, 6.1027, 6.0687, 6.031,
        5.9937, 5.962, 5.9368, 5.9065, 5.875, 5.846, 5.8166, 5.7871, 5.7602,
        5.7368, 5.7154, 5.6956, 5.6825, 5.6649, 5.646, 5.6221, 5.6003,
        5.5817, 5.5626, 5.5448, 5.53, 5.5169, 5.5023, 5.4849, 5.4694,
        5.4558, 5.4441, 5.4327, 5.4233, 5.407, 5.3949, 5.3823, 5.3714,
        5.3596, 5.3471, 5.337, 5.3274, 5.3178, 5.3085, 5.2984, 5.2903,
        5.2829, 5.2766, 5.2691, 5.2611, 5.2532, 5.2457, 5.239, 5.2325,
        5.2267, 5.2202, 5.2143, 5.2095, 5.2046, 5.1997, 5.1959, 5.1918,
        5.1887, 5.1845, 5.1818, 5.1782, 5.1755, 5.1723, 5.171, 5.1685,
        5.1663, 5.1655, 5.1637, 5.162, 5.1613, 5.1603, 5.1594, 5.1584,
        5.1576, 5.1569, 5.1566, 5.1567, 5.1551, 5.1554, 5.1559, 5.1548,
        5.1549
    ],
    "train_ppl": [
        3538.45, 1473.59, 1200.31, 1009.01, 879.87, 793.4, 733.61, 686.15,
        646.12, 607.88, 573.82, 544.28, 517.06, 491.05, 467.82, 447.06,
        432.14, 416.11, 400.9, 388.38, 378.7, 367.43, 356.02, 345.84,
        335.83, 326.05, 317.41, 310.08, 303.5, 297.56, 293.67, 288.57,
        283.15, 276.47, 270.51, 265.51, 260.5, 255.91, 252.14, 248.86,
        245.27, 241.02, 237.32, 234.11, 231.39, 228.76, 226.63, 222.96,
        220.28, 217.53, 215.17, 212.64, 209.99, 207.9, 205.91, 203.94,
        202.05, 200.02, 198.4, 196.93, 195.71, 194.24, 192.69, 191.17,
        189.74, 188.47, 187.26, 186.18, 184.97, 183.89, 183, 182.11,
        181.21, 180.54, 179.79, 179.24, 178.48, 178.01, 177.36, 176.88,
        176.32, 176.09, 175.65, 175.26, 175.12, 174.81, 174.52, 174.39,
        174.22, 174.06, 173.89, 173.75, 173.63, 173.58, 173.6, 173.32,
        173.37, 173.44, 173.26, 173.28
    ],
    "valid_loss": [
        7.1354, 6.9317, 6.7537, 6.6135, 6.5148, 6.447, 6.4007, 6.3507,
        6.3135, 6.2726, 6.2367, 6.1999, 6.166, 6.1365, 6.1084, 6.0985,
        6.0548, 6.0327, 6.0105, 6.0006, 5.9928, 5.9687, 5.9436, 5.9305,
        5.9189, 5.907, 5.8953, 5.8793, 5.8763, 5.8695, 5.8749, 5.8623,
        5.854, 5.8495, 5.8373, 5.8314, 5.8183, 5.8128, 5.8119, 5.8089,
        5.8073, 5.7935, 5.7958, 5.7957, 5.7893, 5.7962, 5.7915, 5.7862,
        5.7793, 5.7676, 5.762, 5.7569, 5.7506, 5.7446, 5.7393, 5.7399,
        5.7344, 5.7391, 5.7419, 5.7385, 5.7392, 5.7423, 5.736, 5.7339,
        5.7334, 5.7341, 5.7301, 5.7239, 5.7219, 5.7202, 5.7204, 5.719,
        5.717, 5.7161, 5.716, 5.7144, 5.714, 5.7142, 5.7138, 5.7136,
        5.7134, 5.7127, 5.7119, 5.7108, 5.7102, 5.7092, 5.7085, 5.7079,
        5.7078, 5.707, 5.7063, 5.7058, 5.7055, 5.7057, 5.7056, 5.7056,
        5.7057, 5.7057, 5.7058, 5.7057
    ],
    "valid_ppl": [
        1255.62, 1024.28, 857.26, 745.09, 675.08, 630.82, 602.24, 572.89,
        551.98, 529.83, 511.15, 492.71, 476.28, 462.45, 449.61, 445.2,
        426.14, 416.84, 407.69, 403.69, 400.53, 391, 381.32, 376.36,
        372.02, 367.61, 363.34, 357.57, 356.48, 354.07, 355.99, 351.53,
        348.61, 347.05, 342.86, 340.84, 336.39, 334.57, 334.26, 333.24,
        332.72, 328.16, 328.91, 328.87, 326.79, 329.05, 327.49, 325.77,
        323.54, 319.77, 317.98, 316.37, 314.37, 312.49, 310.83, 311.03,
        309.33, 310.78, 311.65, 310.61, 310.81, 311.77, 309.83, 309.17,
        309.01, 309.24, 308.01, 306.11, 305.47, 304.97, 305.01, 304.59,
        303.98, 303.71, 303.7, 303.19, 303.09, 303.13, 303.01, 302.97,
        302.9, 302.69, 302.44, 302.11, 301.92, 301.62, 301.43, 301.23,
        301.21, 300.95, 300.75, 300.62, 300.51, 300.59, 300.54, 300.56,
        300.58, 300.59, 300.59, 300.59
    ]
}

# ==========================================
# 2. Plotting Function
# ==========================================

# ==========================================
# 2. Filtering Function (Extract epochs 10-100)
# ==========================================

# Indices 9 to 100 correspond to Epochs 10 to 100
START_IDX = 9 
END_IDX = 100  

# Create the specific epoch range for the X-axis
epochs_slice = full_epochs[START_IDX:END_IDX]

def get_sliced_data(data_dict, key):
    return data_dict[key][START_IDX:END_IDX]

# ==========================================
# 3. Plotting Function
# ==========================================

def plot_metric(metric_name, std_data, hier_data, weighted_data):
#def plot_metric(metric_name, std_data, residual_y_data):
    plt.figure(figsize=(10, 6))
    
    # Plotting lines using the SLICED data and SLICED epochs
    plt.plot(epochs_slice, std_data, label="Standard Transformer", color='blue', linestyle='-', linewidth=1.5)
    plt.plot(epochs_slice, hier_data, label="Hierarchical Transformer (C2P-P2C)", color='green', linestyle='--', linewidth=2.0)
    plt.plot(epochs_slice, weighted_data, label="Hierarchical Transformer (Weighted Sum)", color='orange', linestyle='-.', linewidth=1.5)
    #plt.plot(epochs_slice, residual_y_data, label="Hierarchical Transformer (C2P-P2C with Residual Y)", color='red', linestyle=':', linewidth=2.0)
    
    plt.xlabel("Epoch")
    plt.ylabel(metric_name)
    plt.title(f"{metric_name} vs Epoch (Epochs 10-100)")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Since we are filtering the massive spike, we might NOT need log scale anymore,
    # but let's keep it optional or just linear if the values are close.
    # For 10-90 range, values are much closer, so linear scale often works better to see small differences.
    # However, if you still want log scale for PPL, uncomment the next two lines:
    if "PPL" in metric_name:
        plt.yscale('log')
        
    plt.tight_layout()
    plt.show()

# ==========================================
# 4. Generate the Four Plots (Sliced 10-90)
# ==========================================

# 1. Train Loss
plot_metric("Train Loss",
            get_sliced_data(standard_transformer, "train_loss"),
            get_sliced_data(hierarchical_transformer_C2P_P2C, "train_loss"),
            get_sliced_data(hierarchical_transformer_weighted_sum, "train_loss"))

# 2. Train PPL 
plot_metric("Train PPL",
            get_sliced_data(standard_transformer, "train_ppl"),
            get_sliced_data(hierarchical_transformer_C2P_P2C, "train_ppl"),
            get_sliced_data(hierarchical_transformer_weighted_sum, "train_ppl"))

# 3. Valid Loss
plot_metric("Valid Loss",
            get_sliced_data(standard_transformer, "valid_loss"),
            get_sliced_data(hierarchical_transformer_C2P_P2C, "valid_loss"),
            get_sliced_data(hierarchical_transformer_weighted_sum, "valid_loss"))

# 4. Valid PPL 
plot_metric("Valid PPL",
            get_sliced_data(standard_transformer, "valid_ppl"),
            get_sliced_data(hierarchical_transformer_C2P_P2C, "valid_ppl"),
            get_sliced_data(hierarchical_transformer_weighted_sum, "valid_ppl"))
